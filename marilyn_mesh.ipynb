{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"brad_front.png\")\n",
    "output = image.copy()\n",
    "\n",
    "# Run FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "with mp_face_mesh.FaceMesh(static_image_mode=True) as face_mesh:\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_face_landmarks:\n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "        ih, iw, _ = image.shape\n",
    "\n",
    "        # Draw mesh\n",
    "        for i, j in mp_face_mesh.FACEMESH_TESSELATION:\n",
    "            x1 = int(landmarks[i].x * iw)\n",
    "            y1 = int(landmarks[i].y * ih)\n",
    "            x2 = int(landmarks[j].x * iw)\n",
    "            y2 = int(landmarks[j].y * ih)\n",
    "            cv2.line(output, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "\n",
    "# Show or save result\n",
    "cv2.imshow(\"Face Mesh Overlay\", output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# Optionally save:\n",
    "# cv2.imwrite(\"face_mesh_overlay.png\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525487fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@10.578] global loadsave.cpp:268 findDecoder imread_('brad_front.png'): can't open/read file: check file path/integrity\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745237825.254198   27233 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745237825.265508   27729 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745237825.271541   27727 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m mp_face_mesh \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mface_mesh\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp_face_mesh\u001b[38;5;241m.\u001b[39mFaceMesh(static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m face_mesh:\n\u001b[0;32m---> 10\u001b[0m     results \u001b[38;5;241m=\u001b[39m face_mesh\u001b[38;5;241m.\u001b[39mprocess(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Extract 3D landmarks\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_face_landmarks:\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "%matplotlib qt\n",
    "# Load image and run FaceMesh\n",
    "image = cv2.imread(\"brad_front.png\")\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "with mp_face_mesh.FaceMesh(static_image_mode=True) as face_mesh:\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Extract 3D landmarks\n",
    "if results.multi_face_landmarks:\n",
    "    landmarks = results.multi_face_landmarks[0].landmark\n",
    "    vertices = [(lm.x, lm.y, lm.z) for lm in landmarks]\n",
    "\n",
    "# Get mesh connections\n",
    "connections = mp_face_mesh.FACEMESH_TESSELATION\n",
    "faces = [(i, j, j) for (i, j) in connections]  # approximate triangulation\n",
    "\n",
    "# Prepare data for 3D plotting\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot points\n",
    "xs, ys, zs = zip(*vertices)\n",
    "ax.scatter(xs, ys, zs, color='lightblue', s=0.5)\n",
    "\n",
    "# Plot faces\n",
    "mesh = [[vertices[i], vertices[j], vertices[k]] for i, j, k in faces]\n",
    "ax.add_collection3d(Poly3DCollection(mesh, facecolor='lightgrey', edgecolor='black', linewidths=0.05, alpha=0.5))\n",
    "\n",
    "# Adjust axes\n",
    "ax.view_init(elev=10, azim=90)\n",
    "ax.set_box_aspect([1, 1, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918b4a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@22.108] global loadsave.cpp:268 findDecoder imread_('brad_front.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# === LOAD IMAGE ===\u001b[39;00m\n\u001b[1;32m     12\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[1;32m     14\u001b[0m h, w, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# === INIT MEDIAPIPE ===\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 01:20:20.026 python[3672:27233] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-22 01:20:20.026 python[3672:27233] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# === SETTINGS ===\n",
    "image_path = \"brad_front.png\"\n",
    "obj_path = \"face_mesh.obj\"\n",
    "mtl_path = \"material.mtl\"\n",
    "texture_name = os.path.basename(image_path)\n",
    "\n",
    "# === LOAD IMAGE ===\n",
    "image = cv2.imread(image_path)\n",
    "output = image.copy()\n",
    "h, w, _ = image.shape\n",
    "\n",
    "# === INIT MEDIAPIPE ===\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "face_mesh.close()\n",
    "\n",
    "# === PROCESS LANDMARKS ===\n",
    "if results.multi_face_landmarks:\n",
    "    landmarks = results.multi_face_landmarks[0].landmark\n",
    "    vertices = [(lm.x, lm.y, lm.z) for lm in landmarks]\n",
    "    faces = list(mp_face_mesh.FACEMESH_TESSELATION)\n",
    "\n",
    "    # === DRAW MESH ===\n",
    "    for i, j in faces:\n",
    "        pt1 = (int(landmarks[i].x * w), int(landmarks[i].y * h))\n",
    "        pt2 = (int(landmarks[j].x * w), int(landmarks[j].y * h))\n",
    "        cv2.line(output, pt1, pt2, (0, 255, 0), 1)\n",
    "\n",
    "    # === SAVE OBJ ===\n",
    "    with open(obj_path, \"w\") as f:\n",
    "        f.write(\"mtllib material.mtl\\n\")\n",
    "        for x, y, z in vertices:\n",
    "            f.write(f\"v {x:.6f} {y:.6f} {z:.6f}\\n\")\n",
    "        for x, y, _ in vertices:\n",
    "            f.write(f\"vt {x:.6f} {1 - y:.6f}\\n\")  # Flip y for UV\n",
    "        f.write(\"usemtl face_texture\\n\")\n",
    "        for i, j in faces:\n",
    "            i1, i2, i3 = i + 1, j + 1, j + 1\n",
    "            f.write(f\"f {i1}/{i1} {i2}/{i2} {i3}/{i3}\\n\")\n",
    "\n",
    "    # === SAVE MTL ===\n",
    "    with open(mtl_path, \"w\") as f:\n",
    "        f.write(\"newmtl face_texture\\n\")\n",
    "        f.write(f\"map_Kd {texture_name}\\n\")\n",
    "\n",
    "    # === SAVE IMAGE ===\n",
    "    cv2.imwrite(\"face_mesh_overlay.png\", output)\n",
    "    print(\"✅ OBJ, MTL, and overlay image saved.\")\n",
    "else:\n",
    "    print(\"❌ No face detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6449170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745098500.897530  108564 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "W0000 00:00:1745098500.901167  123357 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745098500.907474  123357 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Load image\n",
    "image_path = \"samples/brad_front.png\"\n",
    "image = cv2.imread(image_path)\n",
    "h, w, _ = image.shape\n",
    "\n",
    "# Run MediaPipe FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "with mp_face_mesh.FaceMesh(static_image_mode=True) as face_mesh:\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if not results.multi_face_landmarks:\n",
    "        raise Exception(\"No face landmarks found.\")\n",
    "\n",
    "landmarks = results.multi_face_landmarks[0].landmark\n",
    "vertices = [(lm.x, lm.y, lm.z) for lm in landmarks]\n",
    "uvs = [(lm.x, 1 - lm.y) for lm in landmarks]  # Flip Y for Blender UV\n",
    "\n",
    "# Get mesh connections\n",
    "connections = mp_face_mesh.FACEMESH_TESSELATION\n",
    "faces = [(i, j, j) for (i, j) in connections]  # simple triangulation\n",
    "\n",
    "# Write .obj\n",
    "with open(\"face.obj\", \"w\") as f:\n",
    "    f.write(\"mtllib face.mtl\\nusemtl face_texture\\n\")\n",
    "    for v in vertices:\n",
    "        f.write(f\"v {v[0]} {v[1]} {v[2]}\\n\")\n",
    "    for uv in uvs:\n",
    "        f.write(f\"vt {uv[0]} {uv[1]}\\n\")\n",
    "    for i, j, k in faces:\n",
    "        f.write(f\"f {i+1}/{i+1} {j+1}/{j+1} {k+1}/{k+1}\\n\")\n",
    "\n",
    "# Write .mtl\n",
    "with open(\"face.mtl\", \"w\") as f:\n",
    "    f.write(\n",
    "        \"\"\"newmtl face_texture\n",
    "Ka 1.000 1.000 1.000\n",
    "Kd 1.000 1.000 1.000\n",
    "Ks 0.000 0.000 0.000\n",
    "d 1.0\n",
    "illum 2\n",
    "map_Kd texture.png\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "# Save texture\n",
    "cv2.imwrite(\"texture.png\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_obj(filename):\n",
    "    vertices = []\n",
    "    faces = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"v \"):  # vertex\n",
    "                _, x, y, z = line.strip().split()\n",
    "                vertices.append((float(x), float(y), float(z)))\n",
    "            elif line.startswith(\"f \"):  # face\n",
    "                parts = line.strip().split()[1:]\n",
    "                face = [int(p.split(\"/\")[0]) - 1 for p in parts]\n",
    "                faces.append(face)\n",
    "\n",
    "    return vertices, faces\n",
    "\n",
    "\n",
    "# Load .obj file\n",
    "verts, faces = load_obj(\"face_mesh.obj\")\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "mesh = [[verts[idx] for idx in face] for face in faces]\n",
    "ax.add_collection3d(\n",
    "    Poly3DCollection(\n",
    "        mesh, facecolor=\"lightgrey\", edgecolor=\"black\", linewidths=0.05, alpha=0.9\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set view\n",
    "xs, ys, zs = zip(*verts)\n",
    "ax.scatter(xs, ys, zs, s=0.2, c=\"r\")\n",
    "ax.set_box_aspect([1, 1, 1])\n",
    "ax.view_init(elev=10, azim=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745081360.246438   86654 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745081360.261604   91455 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745081360.268501   91458 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745081360.271914   91458 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"brad_front.png\")\n",
    "h, w, _ = image.shape\n",
    "\n",
    "# Run FaceMesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "with mp_face_mesh.FaceMesh(static_image_mode=True) as face_mesh:\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Extract landmarks\n",
    "if results.multi_face_landmarks:\n",
    "    landmarks = results.multi_face_landmarks[0].landmark\n",
    "    vertices = [(lm.x, lm.y, lm.z) for lm in landmarks]\n",
    "\n",
    "# Get vertex colours from image\n",
    "colours = []\n",
    "for lm in landmarks:\n",
    "    px = min(w - 1, max(0, int(lm.x * w)))\n",
    "    py = min(h - 1, max(0, int(lm.y * h)))\n",
    "    b, g, r = image[py, px]\n",
    "    colours.append((r/255, g/255, b/255))  # Convert to RGB normalised\n",
    "\n",
    "# Mesh faces\n",
    "connections = mp_face_mesh.FACEMESH_TESSELATION\n",
    "faces = [(i, j, j) for (i, j) in connections]  # Approximate\n",
    "\n",
    "# 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot mesh\n",
    "for i, j, k in faces:\n",
    "    verts = [vertices[i], vertices[j], vertices[k]]\n",
    "    face_col = np.mean([colours[i], colours[j], colours[k]], axis=0)\n",
    "    ax.add_collection3d(Poly3DCollection([verts], facecolors=[face_col], edgecolors='none', linewidths=0.0, alpha=1.0))\n",
    "\n",
    "# Adjust\n",
    "ax.set_box_aspect([1, 1, 1])\n",
    "ax.view_init(elev=10, azim=90)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
